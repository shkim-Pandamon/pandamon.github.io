---
title: "Research Reviews"
excerpt: "reviews"

tages:
    - Reviews
last_modified_at: 2001-01-01T19:25:00
---

Belows are my reviews of several technologies about `Machine Learning`, `AI`, and etc. They could be from research papers, or blog posts. Early period of my graduate student life, I just read articles, highlighted some sentences, saved in on EndNote and, totally forgot everything. And one day, I realized that these could be very volatile, and I cannot remember any knowledge that I studied before. Since then, I tried remaining concise notes that describes the content of that I studied. But still it is not enough to remain the knowledge. Yes, the problem was that I just skim the concepts of it, but did not really 'learn' it and make it as mine. It should be based on 'experience'.  

That is why I begin this reviews. Here, I would try to remain the knowledge in my words and study some toy examples with logs on `GitHub`. Also, I am gonna try to study densely, which means if I do not clearly understand the basis, I would go back to elemental study first and come back to target articles.  

Glad to share my experience. 

> - The order of list is inversely along to the date that I wrote the reviews.
> - [date_of_original_material] name_of_original_material

--------

<details>
<summary><span style="color:black"><strong>Google AI Blog</strong></span></summary>
<div markdown="1">

- [Mar 25. 2021] [Constructing Transformers For Longer Sequences with Sparse Attention Methods - 2]({{variables.base_url}}/reviews/review_constructing_transformers_for_longer_sequences_with_sparse_attention_methods_1_bigbird/). ([original_text](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html) and [original_paper](https://arxiv.org/abs/2007.14062))
- [Mar 25. 2021] [Constructing Transformers For Longer Sequences with Sparse Attention Methods - 1]({{variables.base_url}}/reviews/review_constructing_transformers_for_longer_sequences_with_sparse_attention_methods_1_etc/). ([original_text](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html) and [original_paper](https://arxiv.org/abs/2004.08483))
- [Aug 31. 2017] [Transformer: A Novel Neural Network Architecture for Language Understanding]({{variables.base_url}}/reviews/review_transformer_novel_neural_network_architecture_for_language_understanding/). ([original_text](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) and [original_paper](https://arxiv.org/abs/1706.03762))
- [Sep 27. 2016] [A Neural Network for Machine Translation, at Production Scale]({{variables.base_url}}/reviews/review_neural_network_for_machine_translation_at_production_scale/). ([original_text](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html) and [original_paper](https://arxiv.org/abs/1609.08144))


</div>
</details>